---
output:
  pdf_document: default
  html_document: default
---

# Data treatment, overview, analyse and programming basics

## Data managment

In this part we will see how to import data sets from various nature, clean them, organised then and showing the essential info about them. These treatments will be possible with additional packages *dplyr* and *tidyverse*.

### Load data

If importing data seams simple in other software it can be more tricky under R. We will see here how to import more common data such as .txt, .csv or .excel. For the raster and shapefile data that we might use later see.

::: {.infobox .caution data-latex="{caution}"}
**Data cleaning**

R is a really powerful tool and you can perform any transformation, cleaning or import. **BUT**, R is time consuming, even for senior users, for any kind of treatment. For beginners it might take hours to find a solution to prepare your data set as wanted. Therefore, we will advice you to always prepare your data before in more "user friendly" software as *Excel* or *Open office* solution.
:::

**Included data and folder scaning **

First you can load data already included in the R environment and different packages. To do so you can check all data available with `data()` and then call them with `data(name)` and the dataset will be load.

If the Rstudio IDE with the Files windows give you an overview of your project folder and files you can do it manually via `list.files()` and even add the parameter `path = "foldername"` or `pattern = "\\.filetype"` to precise the folder location or the type of data you want to look at.

**.txt, .csv and .xlsx files**
For a reading a .txt file register in a delimited format (",", ";", TAB ...) you can use the `read.delim("filename")` code with this additional parameters `header = TRUE or FALSE` if you have header. `sep = "x"` To choose your delimiter in you file, *x* can be *\t* for tab, *,* for comma or *;* for semicolon. You can also set your decimal separation with `dec = "x"`.

For reading a .csv file you will use the `read.table("filename")`which is quite similar from `read.delim`. You have some other additional parameters such as `na.strings = "NA"` to choose what to put in the row with NA value (We strongly advice to leave it on NA). `skip = n`can be used to skip a *n* number of lines before reading the data.

And finnaly the .xlsx can be read *via* the `read.xlsx` code from the *openxlsx* package. It allows you to select one specific sheet in the file with the name of the sheet in seconde position after the name of the file as `read.xlsx("filename", "sheetname or sheetindex")`. there is some change in the structure of additionnal parameters as seen before. `startRow = n` defines the line where you begin to read the data, and `sep.names = ""` is for specify the delimiter characters.


``` markdown
>data(iris)

> sample_data <- read.delim("./Data/LUCAS-SOIL-2018.txt", header = TRUE, sep = ",")

> sample_loc <- read.table("./Data/Soil_sample_location.csv", header = TRUE, sep = ";")

> ls()
[1] "iris" "sample_data" "sample_loc"
``` 

### Overview on data

First we will load our first data that we will use during the class session within *palmerpenguins* packages. To load the data make `install.package("palmerpenguis")` then load he package with `library(palmerpenguis)` and load the data with `data(penguins)`.

You have 6 essential command for having an overview of your data set :

1) The most important is `str()` it will give you the class of the object and also from its vectors with some of their data.

2) The `head()` command will give you the columns names and the first lines of the data set.

3) The `tail()` command will give you the columns names and the last lines of the data set.

4) The `ncol()` command will give you the number of columns.

5) The `nrow()` command will give you the number of row in the data set.

6) The `summary()` command will give you the basic statistical information about your data (mean, median...).


### Select data

You can select your data by column in two ways :

1) By columns position `df[n1]` and if you want to select more `df[c(n1, n2 ...)]`. 

2) By columns name `df["name"]` and if you want to select more `df[c("name1", "name2" ...)]`.

If you forget the `c()` it will give a specific row and column number. 

::: {.infobox .caution data-latex="{caution}"}
**List and data frame selection**

For a list you will need to put two *[[]]* for selection. For data frame a better way for selection a specific column is by using `df$name` command.
:::

You can also select specific row with `df[n1,]` and `df[c(n1,n2),]` for multiple selections.

### Remove or add data

If you want to remove data from your data set you can either filter (see later) or just remove a entire row or column.

For removing columns `df[,-c(x1, x2...)]` and for removing rows `df[-c(x1, x2...),]`. If you want to remove by name it is more complicated you have to use `df[ , ! names(df) %in% c("name1", "name2")]` or the `subset(df, select = - c(name2, name2))`. You have also more easy solution with the *dplyr* package.

If you want to add new data to your data set you have two different ways :

1) With the classical R function `rbind(df1, df2)` to add rows to your data and `cbind(df1, df2)` for adding columns. But be careful with this method you can also combine data sets with the same amount of columns for `rbind()`and row for `cbind()` otherwise it will display an error message.

2) The second way is more powerful it is the `merge(df1, df2, by =)` function. The `by =` variable will be your columns name or if you want merge others columns then put the value `by = "row.names"`. Then it will depend if you want to keep all your data even the one with no measurement you will have to add the parameter `all = TRUE` . Otherwise, if you want a cross result to already filter you data you can leave it without this parameter (default is FALSE).

**NAs values**

Another important point is the *NAs* values which are missing value (Very very common !). You can spot them with `is.na(df)` which give you a list of binary answer, true and false to the question "is this a missing value?". You can also see how many NAs you have with the command `sum(is.na(df) == TRUE)`. To treat this missing value you have two solutions. First you can simply removed them with `na.omit(df)` but it will removed the related rows. Another solution that can be really useful is to replace this NAs values by a specific value or value. You can use this line of code `df[is.na(df)] = x` where *x* is a variable of the class of the column, it can be a number as *0, -9999* or a text, factor... You can do the same with the *NULL* variables and the `is.null()` function. 

You can also replace any values by another with `replace(df, y, x)` where *y* is the operator (see below) to be replace and *x* is the value of replacement. 
For example you can use `replace(df, df < 0, 0)` will give zero value to all negative data. You can also replace it by any kind of value (eg. mean, median...). **Attention** for replacing a *NA* value by the mean of any calculation on the variable column, you will need to remove put aside the *NA* from this colom when doing it. Therefore, you have to use the `na.rm = TRUE` parameter after your calculation (`mean(), min(), median(), max()`...).

## Vector operation

Operators  are very useful to filter or even to compare data to each another and also for basic programming of function creation.
Without any package R allows several vector operations listed below.

```{r Vector-operation, echo=FALSE, fig.align ="center", out.width = "75%" }
a <- c("==", "!=", ">", "<", ">=", "<=", "&", "|", "!", "%in%")
b <- c("Is egal to?", "Is not egal to?", "Grater than?", "Less than?", "Greater than or equal to?", "Less than or equal to?", "And", "Or", "Not", "Is found in?")

operator_table <- data.frame(cbind(a,b))
knitr::kable(operator_table, col.names = c("Operator", "Meaning"), caption = "Operators in R.", align = "c")
```

Form the Tidyverse package the "pipe" system is often used to optimized you code. Instead of filtering several time with different data set you can add the code `%>%` to simplify you operator. 

## Export your data 

You can export you data set in many ways but the easiest are with the `write.table(df, "df.export", dec = "", sep = "", row.names = , col.names = )` command. You have several parameters : the first one `df` is the name of your file inside the project while the `"df.export"` is the name you want to give to your file with its extension name (.txt. and .csv are the best choices). The `sep = ""` parameter will allows you to choose your delimiter for decimals number (either , or .), the `sep = ""` parameter will give the separator of your columns (`;` or `,` or tab with `/t`) and `row.names =` and `col.names =` are true or false parameters for keeping the columns names and row names inside your export.  

## Exercices

1) Import and analyse the bodendaten_xy and bodendaten files into Rstudio.

2) Merge the two file into bodendaten_final data frame (careful of the missing link).

3) Remove the columns with no values.

4) Put the texture classification ("Bodenart[...]") as a factor.

5) Fill the missing and wrong values (negative) in the data set :
By the mean (of the variable) for the missing values and the negative values of **Mg**.
By zero for the others negatives values in the data set.
By 100 the "Gu[...]" values that are over 100 (it is the texture percentage). 

6) Export the result inside a "Final_data.Rdata" and a "Final_data.csv" with tab delimiter and header names.




